{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4117b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff2b12",
   "metadata": {},
   "source": [
    "# import path and create needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5beb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
      "['2016', '2017']\n",
      "['ATL', 'CLT', 'DEN', 'DFW', 'EWR', 'IAH', 'JFK', 'LAS', 'LAX', 'MCO', 'MIA', 'ORD', 'PHX', 'SEA', 'SFO']\n"
     ]
    }
   ],
   "source": [
    "#path for data import \n",
    "flight_data_path=r'H:\\00ml\\dat2\\2016\\on_time_2016'\n",
    "weather_data_path=r'H:\\00ml\\dat2\\airport_data'\n",
    "flight_path=r'H:\\00ml\\dat2\\cleaned data\\Flight_data\\n_flight'\n",
    "weather_path=r'H:\\00ml\\dat2\\cleaned data\\Weather_data'\n",
    "mergedata_path=r'H:\\00ml\\dat2\\cleaned data\\mergedata\\n_merge'\n",
    "#crating neccessary variable list data for loop\n",
    "year=['2016','2017']\n",
    "month=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\"]\n",
    "airport=[\"ATL\",\"CLT\",\"DEN\",\"DFW\",\"EWR\",\"IAH\",\"JFK\",\"LAS\",\"LAX\",\"MCO\",\"MIA\",\"ORD\",\"PHX\",\"SEA\",\"SFO\"]\n",
    "\n",
    "def round_time(x):\n",
    "    a=x//100\n",
    "    b=x%100\n",
    "    if b>30:\n",
    "        x=(a+1)*100\n",
    "    elif x==2400:\n",
    "        x=0000\n",
    "    else:\n",
    "        x=a*100\n",
    "    return x\n",
    "\n",
    "print(month)\n",
    "print(year)\n",
    "print(airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d115fc",
   "metadata": {},
   "source": [
    "# flight data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014b1cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/flight_data2016_1\n",
      "/flight_data2016_2\n",
      "/flight_data2016_3\n",
      "/flight_data2016_4\n",
      "/flight_data2016_5\n",
      "/flight_data2016_6\n",
      "/flight_data2016_7\n",
      "/flight_data2016_8\n",
      "/flight_data2016_9\n",
      "/flight_data2016_10\n",
      "/flight_data2016_11\n",
      "/flight_data2016_12\n",
      "/flight_data2017_1\n",
      "/flight_data2017_2\n",
      "/flight_data2017_3\n",
      "/flight_data2017_4\n",
      "/flight_data2017_5\n",
      "/flight_data2017_6\n",
      "/flight_data2017_7\n",
      "/flight_data2017_8\n",
      "/flight_data2017_9\n",
      "/flight_data2017_10\n",
      "/flight_data2017_11\n",
      "/flight_data2017_12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for r in range(0,len(year)):\n",
    "    for j in range(0,len(month)):\n",
    "        #print(path+\"/On_Time_On_Time_Performance_\"+year[r]+\"_\"+month[j]+\".csv\")\n",
    "        flight_raw_data = pd.read_csv(flight_data_path+\"/On_Time_On_Time_Performance_\"+year[r]+\"_\"+month[j]+\".csv\",low_memory=False)\n",
    "        flight_requied_data1=flight_raw_data[['FlightDate','Quarter','Year','Month','DayofMonth','DepTime','DepDel15', 'CRSDepTime','DepDelayMinutes',\n",
    "         'OriginAirportID','DestAirportID','ArrTime','CRSArrTime','ArrDel15','ArrDelayMinutes','Origin','Dest']]\n",
    "        pd.reset_option('mode.chained_assignment')\n",
    "        with pd.option_context('mode.chained_assignment',None):\n",
    "            flight_requied_data1.dropna(subset=['DepTime','ArrTime'], inplace=True)\n",
    "        flight_requied_data=flight_requied_data1[flight_requied_data1.Origin.isin(airport)&flight_requied_data1.Dest.isin(airport)]\n",
    "        \n",
    "        flight_requied_data.reset_index(drop=True)\n",
    "        print(\"/flight_data\"+year[r]+\"_\"+month[j])\n",
    "        flight_requied_data1.to_csv(flight_path+\"/flight_data\"+year[r]+\"_\"+month[j]+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4bc67",
   "metadata": {},
   "source": [
    "# weather data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9881d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weather_data2016_1.csv\n"
     ]
    }
   ],
   "source": [
    "for u in range(0,len(airport)):   #iterate airport name for loop\n",
    "    for r in range(0,len(year)):    #iterate year for loop\n",
    "        for j in range(0,len(month)):     #iterate month for loop\n",
    "            \n",
    "            raw_weather_data = pd.read_json(weather_data_path+\"/\"+airport[u]+\"/\"+year[r]+\"-\"+month[j]+\".json\").T\n",
    "            \n",
    "            # import raw weather data for cleaning and editing \n",
    "            base_weather_data=pd.DataFrame(raw_weather_data['weather'][0])\n",
    "            #extact required data inside the list\n",
    "            request_data=pd.DataFrame(raw_weather_data['request'][0])\n",
    "            #extact required data inside the list\n",
    "            airport_name=request_data['query']\n",
    "            airport_name1=airport_name[0].split(',')[0]\n",
    "            #extact required airport name inside the data list\n",
    "            base_weather_data=base_weather_data[['date','hourly']]\n",
    "            #seperate needed columns from base_weather\n",
    "            base_weather_data.loc[:,'airport']= airport_name1\n",
    "            #seperate airport name from base_weather\n",
    "            daily_data= pd.DataFrame()\n",
    "            #create empty dataframe for append process\n",
    "            for i in range (len(base_weather_data)):\n",
    "                #iterate base weather length for extract nested data\n",
    "                hourly_data=pd.DataFrame(base_weather_data['hourly'][i])\n",
    "                #isolate hourly column nested data\n",
    "                hourly_data=hourly_data[['windspeedKmph', 'winddirDegree', 'weatherCode', 'precipMM', 'visibility', 'pressure', 'cloudcover', 'DewPointF',\n",
    "            'WindGustKmph','tempF', 'WindChillF', 'humidity','time']].astype(float)\n",
    "                #select reqired column from hourly data\n",
    "                date=base_weather_data['date'][i]\n",
    "                #seperate date column from base weather data frame\n",
    "                hourly_data.loc[:,'date']= date\n",
    "                #add date column and its value in hourly data\n",
    "                hourly_data.loc[:,'airport']= airport_name1\n",
    "                #add airport column and its value in hourly data\n",
    "                daily_data=daily_data.append(hourly_data,ignore_index=True)\n",
    "            \n",
    "            print(\"/weather_data\"+year[r]+\"_\"+month[j]+\".csv\")\n",
    "            #import cleaned weather data to .csv for storage with respected name\n",
    "            daily_data.to_csv(weather_path+\"/\"+airport[u]+\"/base_weather_data\"+\"_\"+airport[u]+\"_\"+year[r]+\"_\"+month[j]+\".csv\",index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf4036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5eaadce",
   "metadata": {},
   "source": [
    "# flight and weather data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253c7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016_1\n",
      "2016_2\n",
      "2016_3\n",
      "2016_4\n",
      "2016_5\n",
      "2016_6\n",
      "2016_7\n",
      "2016_8\n",
      "2016_9\n",
      "2016_10\n",
      "2016_11\n",
      "2016_12\n",
      "2017_1\n",
      "2017_2\n",
      "2017_3\n",
      "2017_4\n",
      "2017_5\n",
      "2017_6\n",
      "2017_7\n",
      "2017_8\n",
      "2017_9\n",
      "2017_10\n",
      "2017_11\n",
      "2017_12\n"
     ]
    }
   ],
   "source": [
    "final_file=pd.DataFrame()\n",
    "for r in range(0,len(year)):    #iterate year for loop\n",
    "    \n",
    "    for j in range(0,len(month)):   #iterate month for loop\n",
    "        \n",
    "        flight_data = pd.read_csv(flight_path+\"/flight_data\"+year[r]+\"_\"+month[j]+\".csv\") \n",
    "        # import flight data to df by reading path\n",
    "        flight_data.sort_values(by=['FlightDate', 'DepTime'], inplace=True)  #order data for visulaization\n",
    "        #print(flight_data['DepTime'].head(20))\n",
    "        #drop null value rows which dosen't provide needed information\n",
    "        flight_data['DepTimeh']=flight_data['DepTime'].apply(round_time)\n",
    "        flight_data['ArrTimeh']=flight_data['ArrTime'].apply(round_time)\n",
    "        #print(flight_data['DepTimeh'].head(20))\n",
    "        #convert new time data in to 24 hrs format by replacing 24:00 into 00:00 and continue time in float value to reduce coplication in merging \n",
    "        origin_flight=flight_data.drop('Dest',axis=1)#creating seperate df for origin flight data\n",
    "        dest_flight=flight_data.drop('Origin',axis=1)#creating seperate df for destination flight data\n",
    "        origin_data=pd.DataFrame()\n",
    "        dest_data=pd.DataFrame()\n",
    "        flight_weather=pd.DataFrame()\n",
    "        \n",
    "        #create empty data frame helpfull for merge and append operation\n",
    "        for u in range(0,len(airport)):   #iterate airport name for loop\n",
    "            \n",
    "            weather_data = pd.read_csv(weather_path+\"/\"+airport[u]+\"/base_weather_data\"+\"_\"+airport[u]+\"_\"+year[r]+\"_\"+month[j]+\".csv\")\n",
    "            # import weather data to df by reading path\n",
    "            origin_weather =pd.merge(origin_flight,weather_data,how='inner',left_on=['FlightDate','Origin','DepTimeh'], right_on=['date','airport','time'])\n",
    "            # merging orgin flight with weather data by common given columns and produce inersection valuea as origin data\n",
    "            dest_weather =pd.merge(dest_flight,weather_data,how='inner',left_on=['FlightDate','Dest','ArrTimeh'], right_on=['date','airport','time'])\n",
    "            # merging destination flight with weather data by common given columns and produce inersection valuea as dest data\n",
    "            origin_data=origin_data.append(origin_weather, ignore_index=True)\n",
    "            # append origin weather data to combine 15 airport datas in to single Data frame(origin_data)\n",
    "            dest_data=dest_data.append(dest_weather, ignore_index=True)\n",
    "            # append dest​ weather data to combine 15 airport datas in to single Data frame(dest_data)\n",
    "       \n",
    "\n",
    "        columns =['FlightDate', 'Quarter', 'Year', 'Month', 'DayofMonth', 'DepTime','DepDel15', 'CRSDepTime', 'DepDelayMinutes', 'OriginAirportID','DestAirportID', 'ArrTime', 'CRSArrTime', 'ArrDel15', 'ArrDelayMinutes', 'DepTimeh', 'ArrTimeh']\n",
    "        flight_weather =pd.merge(origin_data,dest_data,how='inner',on=columns,suffixes=('_Origin','_Dest'))\n",
    "        #merging origin_data and dest_data with common columns in columns file to produce intersection value as data frame (flight_weather)\n",
    "        Coln_drop=['date_Origin','time_Origin','Origin','Dest','ArrTimeh','DepTimeh','date_Dest','time_Dest','airport_Origin','airport_Dest']\n",
    "        flight_weather=flight_weather.drop(Coln_drop,axis=1)\n",
    "        #drop unwanted columns\n",
    "        flight_weather=flight_weather.replace('',np.NaN)\n",
    "        flight_weather=flight_weather.dropna()\n",
    "        print(year[r]+\"_\"+month[j])\n",
    "        #import flight_weather data to .csv for storage with respected name\n",
    "        #flight_weather.to_csv(mergedata_path+\"/flight_weather_data\"+year[r]+\"_\"+month[j]+\".csv\",index=False)\n",
    "        final_file=final_file.append(flight_weather, ignore_index=True)\n",
    "        \n",
    "final_file.to_csv(mergedata_path+\"/final_file_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bea37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e4cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ae6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
